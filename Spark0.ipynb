{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepds/sparkcourse/blob/master/Spark0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fSo2Xny-Lci",
        "colab_type": "code",
        "outputId": "4077bc5c-96b7-4545-c9e7-d337941e802e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (2.4.4)\n",
            "Requirement already satisfied: py4j==0.10.7 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdnsqKWO-cVO",
        "colab_type": "text"
      },
      "source": [
        "### 1. Основы Big Data\n",
        "\n",
        "Что такое Big Data? Определение варьируется в зависимости от области применения. Это технология обработки больших массивов данных, которые нельзя обработать традиционными data processing software. \n",
        "\n",
        "Концепция 3 V:\n",
        "1. Volume (объем данных)\n",
        "2. Velocity (скорость передачи, обновления, накопления)\n",
        "3. Variety (разные истоники и формат данных)\n",
        "\n",
        "Термины:\n",
        "1. Кластерные вычисления: объединение ресурсов нескольких машин\n",
        "2. Параллельные вычисления: одновременные вычисления \n",
        "3. Распределенные вычисления: коллекция узлов (сетевых компьютеров), которые работают параллельно\n",
        "4. Пакетная обработка: разбить работу на мелкие кусочки и запустить их на отдельных машинах\n",
        "5. Обработка в реальном времени: real-time обработка данных\n",
        "\n",
        "Системы обработки больших данных:\n",
        "1. Hadoop / MapReduce: масштабируемая и отказоустойчивая среда написана на Java. Открытый исходный код. Пакетная обработка\n",
        "2. Apache Spark: универсальная и быстрая кластерная вычислительная система. Открытый исходный код. Обработка данных как в пакетном режиме, так и в режиме реального времени\n",
        "\n",
        "Преимущества Apache Spark:\n",
        "\n",
        "1. Распределенные вычисления \n",
        "2. Эффективные in-memory вычисления для больших датасетов\n",
        "3. Акцент на быстродействие\n",
        "4. Поддержка Java, Scala, Python, R\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2nECFWL_IRZ",
        "colab_type": "text"
      },
      "source": [
        "Компоненты Apache Spark\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1SQ1q2GnfR4LynFjSj6YGIBj6g5FXPMsR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fPj1wLV_eUr",
        "colab_type": "text"
      },
      "source": [
        "Локальный режим: один компьютер, например, ноутбук\n",
        "\n",
        "Локальная модель удобна для тестирования, отладки и демонстрации\n",
        "\n",
        "Режим кластера: набор предопределенных машин\n",
        "\n",
        "Хорошо для продуктива\n",
        "\n",
        "Работа: Локальная -> кластеры\n",
        "\n",
        "**Нет необходимости менять код!!!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWBBSm6mAcH8",
        "colab_type": "text"
      },
      "source": [
        "Что такое Spark Shell?\n",
        "\n",
        "Интерактивная среда для выполнения заданий Spark (джобов)\n",
        "\n",
        "Полезно для быстрого интерактивного прототипирования\n",
        "\n",
        "Spark Shell позволяют взаимодействовать с данными на диске или в памяти\n",
        "\n",
        "Три разных корпуса Spark:\n",
        "1. Spark-shell для Scala\n",
        "2. PySpark-оболочка для Python\n",
        "3. SparkR для R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ6ZA0l08M5F",
        "colab_type": "text"
      },
      "source": [
        "### 1. Понимание SparkContext и Parallelize\n",
        "\n",
        "SparkContext представляет точку входа в функциональность Spark. Это как ключ к машине."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiO5yLVoHAgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"First App\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSx-MEfY-bfT",
        "colab_type": "code",
        "outputId": "b041c242-9415-4327-8896-5e9f88904958",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Print the version of SparkContext\n",
        "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
        "\n",
        "# Print the Python version of SparkContext\n",
        "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
        "\n",
        "# Print the master of SparkContext\n",
        "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The version of Spark Context in the PySpark shell is 2.4.4\n",
            "The Python version of Spark Context in the PySpark shell is 3.6\n",
            "The master of Spark Context in the PySpark shell is local\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw1jiVRzDSit",
        "colab_type": "text"
      },
      "source": [
        "Для распараллеливания коллекций в программе Driver Spark предоставляет метод SparkContext.parallelize(). При применении метода распараллеливания в коллекции (с элементами) создается новый распределенный набор данных с указанным числом разделов, а элементы коллекции копируются в распределенный набор данных (RDD)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFmqMoAK86i6",
        "colab_type": "code",
        "outputId": "7a99ae1d-10c8-45a9-fb66-23e4ffe7783a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Create a python list of numbers from 1 to 100 \n",
        "numb = range(1, 100)\n",
        "\n",
        "# Load the list into PySpark  \n",
        "spark_data = sc.parallelize(numb)\n",
        "\n",
        "# Load a local file into PySpark shell\n",
        "lines = sc.textFile(file_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-521d9683a7cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load a local file into PySpark shell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'file_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuhJUArpCj8U",
        "colab_type": "text"
      },
      "source": [
        "### 2. Lambda, map и filter методы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5lh6UWJCi9Y",
        "colab_type": "code",
        "outputId": "289a959f-86e0-4870-e219-aecaaf048494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "my_list = numb\n",
        "\n",
        "# Print my_list in the console\n",
        "print(\"Input list is\", my_list)\n",
        "\n",
        "# Square all numbers in my_list\n",
        "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
        "\n",
        "# Print the result of the map function\n",
        "print(\"The squared numbers are\", squared_list_lambda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input list is range(1, 100)\n",
            "The squared numbers are [1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625, 676, 729, 784, 841, 900, 961, 1024, 1089, 1156, 1225, 1296, 1369, 1444, 1521, 1600, 1681, 1764, 1849, 1936, 2025, 2116, 2209, 2304, 2401, 2500, 2601, 2704, 2809, 2916, 3025, 3136, 3249, 3364, 3481, 3600, 3721, 3844, 3969, 4096, 4225, 4356, 4489, 4624, 4761, 4900, 5041, 5184, 5329, 5476, 5625, 5776, 5929, 6084, 6241, 6400, 6561, 6724, 6889, 7056, 7225, 7396, 7569, 7744, 7921, 8100, 8281, 8464, 8649, 8836, 9025, 9216, 9409, 9604, 9801]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdmriuG2C40z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print my_list2 in the console\n",
        "print(\"Input list is:\", my_list2)\n",
        "\n",
        "# Filter numbers divisible by 10\n",
        "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
        "\n",
        "# Print the numbers divisible by 10\n",
        "print(\"Numbers divisible by 10 are:\", filtered_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezsLDskfDZ7p",
        "colab_type": "text"
      },
      "source": [
        "### 3. Работа с RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyCUxCBuEts7",
        "colab_type": "text"
      },
      "source": [
        "### RDD из распараллеленных коллекций\n",
        "\n",
        "Resilient Distributed Dataset (RDD) - основная абстракция в Spark. Это неизменяемая распределенная коллекция объектов. Поскольку RDD является фундаментальным и базовым типом данных в Spark, важно, чтобы вы понимали, как его создать. В этом упражнении вы создадите свой первый RDD в PySpark из набора слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyPc5-ZDDbgO",
        "colab_type": "code",
        "outputId": "098100af-e82c-4c9a-8c17-0849696f111d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Create an RDD from a list of words\n",
        "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
        "\n",
        "# Print out the type of the created object\n",
        "print(\"The type of RDD is\", type(RDD))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The type of RDD is <class 'pyspark.rdd.RDD'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqr66ySoFN2c",
        "colab_type": "text"
      },
      "source": [
        "### RDD из внешних наборов данных\n",
        "\n",
        "PySpark может легко создавать RDD из файлов, которые хранятся на внешних устройствах хранения, таких как HDFS (распределенная файловая система Hadoop), корзины Amazon S3 и т. Д. Однако наиболее распространенный метод создания RDD - это файлы, хранящиеся в локальной файловой системе. Этот метод берет путь к файлу и читает его как набор строк. В этом упражнении вы создадите RDD из пути к файлу (file_path) с именем файла README.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AFDkcYeFV3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the file_path\n",
        "print(\"The file_path is\", file_path)\n",
        "\n",
        "# Create a fileRDD from file_path\n",
        "fileRDD = sc.textFile(file_path)\n",
        "\n",
        "# Check the type of fileRDD\n",
        "print(\"The file type of fileRDD is\", type(fileRDD))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfJ9nqWNFwmI",
        "colab_type": "text"
      },
      "source": [
        "### Партицирование ваших данных\n",
        "\n",
        "Метод textFile() SparkContext принимает необязательный второй аргумент minPartitions для указания минимального количества партиций. В этом упражнении вы создадите RDD с именем fileRDD_part с 5 разделами, а затем сравните его с fileRDD, созданным в предыдущем упражнении."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqiqfPMQGOLO",
        "colab_type": "code",
        "outputId": "7793fb63-96bf-4581-d929-cb04a1c6fca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 97kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 35.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=9fbec0fd84063078f46e31428c13270229cd8c5f295c2350ab7f6d2e690ab87c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NlQ0CqBF7Rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check the number of partitions in fileRDD\n",
        "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions()) \n",
        "\n",
        "# Create a fileRDD_part from file_path with 5 partitions\n",
        "fileRDD_part = sc.textFile(file_path, minPartitions = 6)\n",
        "\n",
        "# Check the number of partitions in fileRDD_part\n",
        "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0YW4IDxDErl",
        "colab_type": "text"
      },
      "source": [
        "### Трансформации и действия\n",
        "\n",
        "Все операции на RDD делятся на два вида - трансформации и действия. При трансформациях создается новый RDD, при действии - на выходе число. Все трансформации подвержены lazy вычислениям, осуществляются до первого действия. При операциях с RDD Spark создает вычислительный граф. Это делает Spark высокоэффективным. Если цепочка трансформаций передается первой трансформации, то логично не вычислять всю цепочку, а использовать первый результат.\n",
        "\n",
        "Трансформации: map, filter, flatMap, union\n",
        "\n",
        "Действия: collect, take, first, count\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkq_0TAJDE25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create map() transformation to cube numbers\n",
        "cubedRDD = numbRDD.map(lambda x: ____)\n",
        "\n",
        "# Collect the results\n",
        "numbers_all = cubedRDD.____()\n",
        "\n",
        "# Print the numbers from numbers_all\n",
        "for numb in ____:\n",
        "\tprint(____)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC0-gkTnD2qM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter the fileRDD to select lines with Spark keyword\n",
        "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in ____) # in line\n",
        "\n",
        "# How many lines are there in fileRDD?\n",
        "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.____())\n",
        "\n",
        "# Print the first four lines of fileRDD\n",
        "for line in fileRDD_filter.____(____): \n",
        "  print(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPNShnVFYFRO",
        "colab_type": "text"
      },
      "source": [
        "### Pair RDD\n",
        "\n",
        "Реальные датасеты обычно составляют пары ключ-значение. Два способа создания Pair RDD: из кортежа и из regular RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqYPA5J3YHL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
        "pairRDD_tuple = sc.parallelize(my_tuple)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Muotn6NXZoYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_list = ['Sam 23', 'Mary 24', 'Peter 25']\n",
        "regularRDD = sc.parallelize(my_list)\n",
        "pairRDD_RDD = regularRDD.map(lambda s: (s.split(\" \")[0], s.split(\" \")[1])) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sknoKiMAbEJW",
        "colab_type": "text"
      },
      "source": [
        "Функции, применимые к pairRDD \n",
        "\n",
        "reduceByKey() # combine values with the same key\n",
        "\n",
        "groupByKey() # group values with the same key\n",
        "\n",
        "sortByKey() # return rdd sorted by the key\n",
        "\n",
        "join() # join two pair rdd by key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQp-gbTcbf2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regular_RDD = sc.parallelize([(\"Messi\", 23), (\"Neymar\", 24), (\"Ronaldo\", 34), (\"Messi\", 24)])\n",
        "pair_RDD_reducedbykey = regularRDD.reduceByKey(lambda x,y: x+y)\n",
        "pair_RDD_reducedbykey.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMIKy7zOdZ46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pair_RDD_reducedbykey_rev = pairRDD_reducedbykey.map(lambda x: (x[1], x[0]))\n",
        "pair_RDD_reducedbykey_rev.sortByKey(ascending=False).collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZui7D9qg446",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "airports = [(\"UK\", \"JFK\"), (\"US\", \"LHR\"), (\"UK\", \"UIY\"), (\"US\", \"KUJ\")]\n",
        "regular_RDD = sc.parallelize(airports)\n",
        "pairRDD_group = regular_RDD.groupByKey().collect()\n",
        "\n",
        "for cont, air in pairRDD_group:\n",
        "  print(cont, list(air))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxnBlZZ6hrP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RDD1 = sc.parallelize([(\"Messi\", 45), (\"Ronaldo\", 23), (\"Gerrard\", 34), (\"Zidane\", 100)])\n",
        "RDD2 = sc.parallelize([(\"Messi\", 32), (\"Coman\", 3), ('Ronaldo', 99)]\n",
        "RDD1.join(RDD2).collect()                      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOnoxCVOrLtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create PairRDD Rdd with key value pairs\n",
        "Rdd = sc.parallelize([____])\n",
        "\n",
        "# Apply reduceByKey() operation on Rdd\n",
        "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: ____)\n",
        "\n",
        "# Iterate over the result and print the output\n",
        "for num in Rdd_Reduced.____: \n",
        "  print(\"Key {} has {} Counts\".format(____, num[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A9Kb5nyrUix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort the reduced RDD with the key by descending order\n",
        "Rdd_Reduced_Sort = Rdd_Reduced.____(ascending=False)\n",
        "\n",
        "# Iterate over the result and print the output\n",
        "for num in Rdd_Reduced_Sort.____():\n",
        "  print(\"Key {} has {} Counts\".format(____, num[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIYpQWCfrmZa",
        "colab_type": "text"
      },
      "source": [
        "### Продвинутые операции с RDD\n",
        "\n",
        "Reduce() action\n",
        "\n",
        "saveOfTextFile() # save rdd to a text file, separate file for each partition\n",
        "\n",
        "coalesce() # compose all partitions to a single RDD \n",
        "\n",
        "collectAsMap() # return key-value pairs in the RDD as dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUnZuvFHsScM",
        "colab_type": "code",
        "outputId": "9bc8c212-cbbc-4261-80b8-3d7f834cf166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 1.3MB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 50.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=d281cf2bee253929c691aad2d79c32bd047ec40337fdf304c8320109114a2c9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6waX8tpHGee",
        "colab_type": "code",
        "outputId": "5b4d496c-05cd-4097-d336-a15eaa9143f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pyspark\n",
        "pyspark.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdCP9uQYsFJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\",\"First App\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyNNc0lHrl-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [1,3,4,6]\n",
        "\n",
        "RDD = sc.parallelize(x)\n",
        "\n",
        "RDD.reduce(lambda x,y: x+y)\n",
        "\n",
        "RDD.saveAsTextFile(\"tempFile\")\n",
        "\n",
        "RDD.coalesce(1).saveAsTextFile(\"tempFile\") # save rdd as one text file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxthP8WGxbDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "countByKey() # avalilable for type (k,v), action counts the number of elements for each key"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxAKMSdXss5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rdd = sc.parallelize([(\"a\",1),(\"b\",1),(\"a\",1)])\n",
        "for key, val in rdd.countByKey().items():\n",
        "  print(key,val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlWvm9WISJDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc.parallelize([(1,2),(3,4)]).collectAsMap()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr9wkpZyVfrL",
        "colab_type": "text"
      },
      "source": [
        "Задание. Удалите стоп-слова и уменьшите набор данных\n",
        "\n",
        "После разбиения строк в файле на длинный список слов с помощью преобразования flatMap () на следующем шаге вы удалите стоп слова из ваших данных. \n",
        "Стоп-слова являются общими словами, которые часто неинтересны. Например, «I», «the», «a» и т. Д. Являются стоп-словами. \n",
        "Вы можете удалить много очевидных стоп-слов со своим собственным списком. Но для этого упражнения вы просто удалите стоп-слова из списка курируемых слов stop_words, предоставленных вам в вашей среде.\n",
        "После удаления стоп-слов вы затем создадите пару RDD, где каждый элемент является парным кортежем (k, v), где k - это ключ, а v - это значение. В этом примере pair RDD состоит из (w, 1), где w - для каждого слова в RDD, \n",
        "а 1 - число. Наконец, вы скомбинируете значения с одним и тем же ключом из пары RDD, используя операцию reduByKey ()\n",
        "Помните, что в вашей рабочей области уже есть SparkContext sc и splitRDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWLEibVOSRO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the words in lower case and remove stop words from stop_words\n",
        "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in x)\n",
        "\n",
        "# Create a tuple of the word and 1 \n",
        "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
        "\n",
        "# Count of the number of occurences of each word\n",
        "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkkrG30yWe-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display the first 10 words and their frequencies\n",
        "for word in resultRDD.take(10):\n",
        "\tprint(word)\n",
        "\n",
        "# Swap the keys and values \n",
        "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
        "\n",
        "# Sort the keys in descending order\n",
        "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
        "\n",
        "# Show the top 10 most frequent words and their frequencies\n",
        "for word in resultRDD_swap_sort.take(10):\n",
        "\tprint(\"{} has {} counts\". format(word[1], word[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUPDxxB6fjmy",
        "colab_type": "text"
      },
      "source": [
        "### Dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xuw5sdKfrvP",
        "colab_type": "text"
      },
      "source": [
        "RDD в DataFrame\n",
        "\n",
        "Подобно RDD, DataFrames являются неизменяемыми и распределенными структурами данных в Spark. Несмотря на то, что RDD являются фундаментальной структурой данных в Spark, работа с данными в DataFrame в большинстве случаев проще, чем RDD, и поэтому необходимо понимание того, как преобразовать RDD в DataFrame.\n",
        "\n",
        "В этом упражнении вы сначала создаете RDD, используя sample_list, который содержит список кортежей («Mona», 20), («Jennifer», 34), («John», 20), («Jim», 26 ) с каждым кортежем указывается имя человека и его возраст. Затем создайте DataFrame с использованием RDD и схемы (которая является списком «Name» и «Age») и, наконец,  выведите как PySpark DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THYthnxZgBi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('abc').getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke8gI3cXfk7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a list of tuples\n",
        "sample_list = [('Mona',20), ('Jennifer', 34), ('John', 20), ('Jim', 26)]\n",
        "\n",
        "# Create a RDD from the list\n",
        "rdd = sc.parallelize(sample_list)\n",
        "\n",
        "# Create a PySpark DataFrame\n",
        "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FDNBl3kf8bn",
        "colab_type": "code",
        "outputId": "1f4cc4d9-477a-40f8-a41f-3142aebc35b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "names_df.take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Name='Mona', Age=20)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZXxF1nZgmgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an DataFrame from file_path\n",
        "people_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Check the type of people_df\n",
        "print(\"The type of people_df is\", type(people_df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FTg91AXhDuw",
        "colab_type": "text"
      },
      "source": [
        "### Transformations и Actions \n",
        "\n",
        "select(), filter(), groupby(), orderby(), dropDuplicates(), withColumnRenamed()\n",
        "\n",
        "printSchema(), head(), show(), count(), columns(), describe()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBU1mYxvhFdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the first 10 observations \n",
        "people_df.____(10)\n",
        "\n",
        "# Count the number of rows \n",
        "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.____()))\n",
        "\n",
        "# Count the number of columns and their names\n",
        "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.____), people_df.____))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjAku9WLiy0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select name, sex and date of birth columns\n",
        "people_df_sub = people_df.____('name', ____, ____)\n",
        "\n",
        "# Print the first 10 observations from people_df_sub\n",
        "people_df_sub.____(____)\n",
        "\n",
        "# Remove duplicate entries from people_df_sub\n",
        "people_df_sub_nodup = people_df_sub.____()\n",
        "\n",
        "# Count the number of rows\n",
        "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.____(), people_df_sub_nodup.____()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zI7xqH7jXQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter people_df to select females \n",
        "people_df_female = people_df.____(people_df.____ == \"female\")\n",
        "\n",
        "# Filter people_df to select males\n",
        "people_df_male = people_df.____(____ == \"____\")\n",
        "\n",
        "# Count the number of rows \n",
        "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\".format(people_df_female.____(), people_df_male.____()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2bP-hN_kRPW",
        "colab_type": "text"
      },
      "source": [
        "### Исполнение SQL запросов\n",
        "\n",
        "SparkSession поддерживает выполнение SQL запросов\n",
        "\n",
        "sql() метод принимает SQL запрос и возвращается датафрейм\n",
        "\n",
        "createOrReplaceTempView() метод создает временную табличку из датафрейма"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2clqO5EDkTPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.createOrReplaceTempView(\"table1\")\n",
        "\n",
        "df2 = spark.sql(\"SELECT field1, field2 from table1\")\n",
        "\n",
        "df2.collect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYUnlmLyyTd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a temporary table \"people\"\n",
        "people_df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# Construct a query to select the names of the people from the temporary table \"people\"\n",
        "query = '''SELECT name FROM people'''\n",
        "\n",
        "# Assign the result of Spark's query to people_df_names\n",
        "people_df_names = spark.sql(query)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jYLj58My2pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter the people table to select female sex \n",
        "people_female_df = spark.sql('SELECT * FROM people WHERE sex==\"female\"')\n",
        "\n",
        "# Filter the people table DataFrame to select male sex\n",
        "people_male_df = spark.sql('SELECT * FROM people WHERE sex==\"male\"')\n",
        "\n",
        "# Count the number of rows in both DataFrames\n",
        "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(), people_male_df.count()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJQVCZ4vzE5f",
        "colab_type": "text"
      },
      "source": [
        "### Визуализация датафреймов\n",
        "\n",
        "Matplotlib, seaborn, bokeh\n",
        "\n",
        "1. Pyspark_dist_explore библиотека поддерживает три функции hist(), distplot(), pandas_histogram()\n",
        "2. Использование функций pandas (в основе matplotlib и seaborn) для отрисовки датафреймов (pandas dataframes - находятся в in-memory, значит, что обработка ограниченна мощностями одного сервера), Pyspark dataframes обрабатываются распределенно и lazy. \n",
        "Датафреймы Pandas - лишены этих преимуществ. toPandas() метод.\n",
        "\n",
        "3.HandySpark библиотека для визуализации датафреймов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RABOMdLvzH-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check the column names of names_df\n",
        "print(\"The column names of names_df are\", names_df.____)\n",
        "\n",
        "# Convert to Pandas DataFrame  \n",
        "df_pandas = names_df.____()\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "____.plot(kind='barh', x='____', y='____', colormap='winter_r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx7UFNWL6ycn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Dataframe\n",
        "fifa_df = spark.____(____, header=True, inferSchema=True)\n",
        "\n",
        "# Check the schema of columns\n",
        "fifa_df.____()\n",
        "\n",
        "# Show the first 10 observations\n",
        "fifa_df.____(____)\n",
        "\n",
        "# Print the total number of rows\n",
        "print(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.____()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpQDZMHE8Ocl",
        "colab_type": "text"
      },
      "source": [
        "### PySpark MLlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AbcPUnF8QPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the library for ALS\n",
        "from pyspark.mllib.recommendation import ALS\n",
        "\n",
        "# Import the library for Logistic Regression\n",
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
        "\n",
        "# Import the library for Kmeans\n",
        "from pyspark.mllib.clustering import KMeans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LnlAEwt9yY7",
        "colab_type": "text"
      },
      "source": [
        "### Коллаборативная фильтрация\n",
        "\n",
        "Use-user collaborative filtering - поиск пользователей наболее близких к целевому пользователю\n",
        "\n",
        "Item-item colloborative filtering - поиск товаров, которые наиболее близки к целевому товару\n",
        "\n",
        "(user, product, rating) - структура"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3VbGoTNCvtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.mllib.recommendation import Rating, ALS\n",
        "\n",
        "r = Rating(user = 1, product = 2, rating = 5.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOuOIzig9vW_",
        "colab_type": "code",
        "outputId": "7db3c723-3d87-4038-9aab-ad93462fb126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
        "train, test = data.randomSplit([0.6, 0.4])\n",
        "\n",
        "train.collect\n",
        "test.collect"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method RDD.collect of PythonRDD[25] at RDD at PythonRDD.scala:53>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkRIVttUAtAv",
        "colab_type": "code",
        "outputId": "bbdf4e50-6bc7-4204-dcd0-755d5f3c2e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "r1 = Rating(1,1,1.0)\n",
        "r2 = Rating(1,2,5.0)\n",
        "r3 = Rating(2,1,2.0)\n",
        "ratings = sc.parallelize([r1, r2, r3])\n",
        "ratings.collect"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method RDD.collect of ParallelCollectionRDD[26] at parallelize at PythonRDD.scala:195>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMydmzg8D4PH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ALS.train(ratings, rank=10, iterations=10)\n",
        "\n",
        "unrated_RDD = sc.parallelize([(1,2),(1,1)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zIvxppbD8EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predictAll(unrated_RDD)\n",
        "predictions.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwTqZlDxIJTs",
        "colab_type": "text"
      },
      "source": [
        "Загрузка набора данных кино в RDD\n",
        "\n",
        "Коллаборативная фильтрация - это метод для рекомендательных систем, в котором оценки пользователей и взаимодействия с различными продуктами используются для рекомендации новых. С появлением машинного обучения и параллельной обработки данных рекомендательные системы стали широко популярными в последние годы и используются во многих областях, включая фильмы, музыку, новости, книги, исследовательские статьи, поисковые запросы, социальные теги. В этом упражнении, состоящем из трех частей, ваша цель - разработать простую систему рекомендаций к фильмам с использованием PySpark MLlib с использованием подмножества набора данных MovieLens 100k.\n",
        "\n",
        "В первой части вы сначала загрузите данные MovieLens (ratings.csv) в RDD и из каждой строки в RDD, отформатированной как userId, movieId, rating, timestamp, вам нужно будет сопоставить данные MovieLens с Рейтинговый объект (userID, productID, rating) после удаления столбца меток времени, и, наконец, вы разделите RDD на обучающие и тестовые RDD.\n",
        "\n",
        "Помните, у вас есть SparkContext sc в вашей рабочей области. Также переменная file_path (которая является путем к файлу ratings.csv) и класс ALS уже доступны в вашей рабочей области."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac0uwqJyIJeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data into RDD\n",
        "data = sc.textFile(file_path)\n",
        "\n",
        "# Split the RDD \n",
        "ratings = data.map(lambda l: l.split(','))\n",
        "\n",
        "# Transform the ratings RDD \n",
        "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
        "\n",
        "# Split the data into training and test\n",
        "training_data, test_data = ratings_final.randomSplit([0.8, 0.2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkK9IqDcIzn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the ALS model on the training data\n",
        "model = ALS.train(training_data, rank=10, iterations=10)\n",
        "\n",
        "# Drop the ratings column \n",
        "testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
        "\n",
        "# Predict the model  \n",
        "predictions = model.predictAll(testdata_no_rating)\n",
        "\n",
        "# Print the first rows of the RDD\n",
        "predictions.take(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EGQQRJAJl5R",
        "colab_type": "text"
      },
      "source": [
        "Оценка модели с использованием MSE\n",
        "\n",
        "После создания прогнозируемых оценок на основе тестовых данных с использованием модели ALS в этой заключительной части упражнения вы подготовите данные для расчета среднеквадратической ошибки (MSE) модели. MSE - это среднее значение (исходный рейтинг - прогнозируемый рейтинг) ^ 2 для всех пользователей и указывает абсолютное соответствие модели данным. Чтобы сделать это, вы сначала организуете и рейтинговые и прогнозные RDD, чтобы сделать кортеж ((пользователь, продукт), рейтинг)), затем присоединитесь к рейтинговому RDD с прогнозным RDD и, наконец, примените функцию разности в квадрате вместе со средним (), чтобы получить MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqHob5fGJckc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare ratings data\n",
        "rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))\n",
        "\n",
        "# Prepare predictions data\n",
        "preds = predictions.map(lambda r: ((r[0], r[1]), r[2]))\n",
        "\n",
        "# Join the ratings data with predictions data\n",
        "rates_and_preds = rates.join(preds)\n",
        "\n",
        "# Calculate and print MSE\n",
        "MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
        "print(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7LQeFA0FE7k",
        "colab_type": "text"
      },
      "source": [
        "### Классификация\n",
        "\n",
        "Два типа структур данных в MLlib:\n",
        "\n",
        "1. Vectors \n",
        "2. LabeledPoint\n",
        "\n",
        "Существует два типа векторов:\n",
        "\n",
        "1. Dense vectors # store all entries in array as float numbers\n",
        "2. Sparse vectors # store only non zero values and their indices!!!\n",
        "\n",
        "LabeledPoint - враппер для входящих переменных и целевой функции для дальнейшего использования в моделях\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5XQj0RoFG8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.mllib.linalg import Vectors\n",
        "\n",
        "dense_vec = Vectors.dense([1.0, 2.0, 3.0])\n",
        "sparse_vec = Vectors.sparse(4, {1: 2.0, 3: 5.0}) # 4 is the dimension of vector, 1 и 3 indices of non-zero elements"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1FR9V6lGwzh",
        "colab_type": "code",
        "outputId": "ae12a3d5-ba1b-4518-fe83-7d699a94e54b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sparse_vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseVector(4, {1: 2.0, 3: 5.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaZrEUSnHgED",
        "colab_type": "code",
        "outputId": "6c8d3e35-a780-48dd-8546-fcdfeb6e6b92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from pyspark.mllib.regression import LabeledPoint\n",
        "\n",
        "positive = LabeledPoint(1, [1.0, 2.4, 5.0])\n",
        "negative = LabeledPoint(0, [0.0, -1.0, 3.0])\n",
        "\n",
        "print(positive)\n",
        "print(negative)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1.0,[1.0,2.4,5.0])\n",
            "(0.0,[0.0,-1.0,3.0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSGFdNhII2YA",
        "colab_type": "code",
        "outputId": "53d30614-bc80-4fa3-bf55-0dbc28be1dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from pyspark.mllib.feature import HashingTF #as countvectorizer\n",
        "\n",
        "sentence = \"hello world to all\"\n",
        "\n",
        "words = sentence.split()\n",
        "\n",
        "tf = HashingTF(10000) # countvectorizer version (but there are differencies, see stackoverflow)\n",
        "\n",
        "tf.transform(words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseVector(10000, {2342: 1.0, 3959: 1.0, 4673: 1.0, 9357: 1.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTasBgYcdgnJ",
        "colab_type": "code",
        "outputId": "a1c9f424-f101-4494-a67f-d12d9e7f9bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
        "\n",
        "data = [LabeledPoint(0, [0.0, 1.0]), LabeledPoint(1, [1.0, 0.0])]\n",
        "\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "model = LogisticRegressionWithLBFGS.train(rdd)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-b81a93c3be59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionWithLBFGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cls, data, iterations, initialWeights, regParam, regType, intercept, corrections, tolerance, validateData, numClasses)\u001b[0m\n\u001b[1;32m    400\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                     \u001b[0minitialWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumClasses\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_regression_train_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/mllib/regression.py\u001b[0m in \u001b[0;36m_regression_train_wrapper\u001b[0;34m(train_func, modelClass, data, initial_weights)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodelClass\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         weights, intercept, numFeatures, numClasses = train_func(\n\u001b[0;32m--> 217\u001b[0;31m             data, _convert_to_vector(initial_weights))\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodelClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/mllib/classification.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rdd, i)\u001b[0m\n\u001b[1;32m    390\u001b[0m             return callMLlibFunc(\"trainLogisticRegressionModelWithLBFGS\", rdd, int(iterations), i,\n\u001b[1;32m    391\u001b[0m                                  \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m                                  float(tolerance), bool(validateData), int(numClasses))\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitialWeights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36m_py2java\u001b[0;34m(sc, obj)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;34m\"\"\" Convert Python object into Java \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36m_to_java_object_rdd\u001b[0;34m(rdd)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonToJava\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.mllib.api.python.SerDe.pythonToJava.\n: java.lang.IllegalArgumentException: Unsupported class file major version 55\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:798)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:797)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:797)\n\tat org.apache.spark.mllib.api.python.SerDeBase.pythonToJava(PythonMLLibAPI.scala:1346)\n\tat org.apache.spark.mllib.api.python.SerDe.pythonToJava(PythonMLLibAPI.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1a80Z6ooYst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the datasets into RDDs\n",
        "spam_rdd = sc.textFile(file_path_spam)\n",
        "non_spam_rdd = sc.textFile(file_path_non_spam)\n",
        "\n",
        "# Split the email messages into words\n",
        "spam_words = spam_rdd.flatMap(lambda email: email.split(' '))\n",
        "non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))\n",
        "\n",
        "# Print the first element in the split RDD\n",
        "print(\"The first element in spam_words is\", spam_words.first())\n",
        "print(\"The first element in non_spam_words is\",non_spam_words.first())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKajSmT2qsZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a HashingTf instance with 200 features\n",
        "tf = HashingTF(numFeatures=200)\n",
        "\n",
        "# Map each word to one feature\n",
        "spam_features = tf.transform(spam_words)\n",
        "non_spam_features = tf.transform(non_spam_words)\n",
        "\n",
        "# Label the features: 1 for spam, 0 for non-spam\n",
        "spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
        "non_spam_samples = non_spam_features.map(lambda features:LabeledPoint(0, features))\n",
        "\n",
        "# Combine the two datasets\n",
        "samples = spam_samples.join(non_spam_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2jnzxDDrh4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into training and testing\n",
        "train_samples,test_samples = samples.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegressionWithLBFGS.train(train_samples)\n",
        "\n",
        "# Create a prediction label from the test data\n",
        "predictions = model.predict(test_samples.map(lambda x: x.features))\n",
        "\n",
        "# Combine original labels with the predicted labels\n",
        "labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\n",
        "\n",
        "# Check the accuracy of the model on the test data\n",
        "accuracy = labels_and_preds.filter(lambda x: x[1] == x[1]).count() / float(test_samples.count())\n",
        "print(\"Model accuracy : {:.2f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtHKoLL8sagJ",
        "colab_type": "text"
      },
      "source": [
        "### Кластеризация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIpfB67Gscf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the dataset into a RDD\n",
        "clusterRDD = sc.textFile(file_path)\n",
        "\n",
        "# Split the RDD based on tab\n",
        "rdd_split = clusterRDD.map(lambda x: x.split(\"\\t\"))\n",
        "\n",
        "# Transform the split RDD by creating a list of integers\n",
        "rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])\n",
        "\n",
        "# Count the number of rows in RDD \n",
        "print(\"There are {} rows in the rdd_split_int dataset\".format(rdd_split_int.count()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtnIaOXpui6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model with clusters from 13 to 16 and compute WSSSE \n",
        "for clst in range(13, 17):\n",
        "    model = KMeans.train(rdd_split_int, clst, seed=1)\n",
        "    WSSSE = rdd_split_int.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
        "    print(\"The cluster {} has Within Set Sum of Squared Error {}\".format(clst, WSSSE))\n",
        "\n",
        "# Train the model again with the best k \n",
        "model = KMeans.train(rdd_split_int, k=15, seed=1)\n",
        "\n",
        "# Get cluster centers\n",
        "cluster_centers = model.clusterCenters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwFa98V-ulMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert rdd_split_int RDD into Spark DataFrame\n",
        "rdd_split_int_df = spark.createDataFrame(rdd_split_int, schema=[\"col1\", \"col2\"])\n",
        "\n",
        "# Convert Spark DataFrame into Pandas DataFrame\n",
        "rdd_split_int_df_pandas = rdd_split_int_df.toPandas()\n",
        "\n",
        "# Convert \"cluster_centers\" that you generated earlier into Pandas DataFrame\n",
        "cluster_centers_pandas = pd.DataFrame(cluster_centers, columns=[\"col1\", \"col2\"])\n",
        "\n",
        "# Create an overlaid scatter plot\n",
        "plt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"])\n",
        "plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}